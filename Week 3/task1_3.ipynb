{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(model, image):\n",
    "    # Perform inference\n",
    "    results = model(image, verbose=False)\n",
    "    # Extract bounding box coordinates, labels, and confidence scores\n",
    "    return [box.xyxy[0].to(int).tolist() for box in results[0].boxes if box.cls == 2 or box.cls == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_tracking_points(image, bboxes):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    corners = []\n",
    "    for bbox in bboxes:\n",
    "        # Convert bbox coordinates from float to int and expand out the variables\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        # Extract the region of interest (the detected object)\n",
    "        roi = gray[y1:y2, x1:x2]\n",
    "        # Detect Harris corners in the ROI\n",
    "        corner_points = cv2.goodFeaturesToTrack(roi, maxCorners=100, qualityLevel=0.1, minDistance=3)\n",
    "        if corner_points is not None:\n",
    "            corner_points = np.squeeze(corner_points, axis=1)\n",
    "            # Convert local ROI coordinates to global image coordinates\n",
    "            corner_points[:, 0] += x1\n",
    "            corner_points[:, 1] += y1\n",
    "            corners.append(corner_points)\n",
    "    return corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_objects(prev_frame, current_frame, prev_points):\n",
    "    # Convert frames to grayscale\n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    current_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    prev_points = np.reshape(prev_points, (-1, 1, 2))\n",
    "    # Calculate optical flow (Lucas-Kanade)\n",
    "    current_points, status, error = cv2.calcOpticalFlowPyrLK(prev_gray, current_gray, prev_points, None)\n",
    "    # Select good points (where status == 1)\n",
    "    good_new = current_points[status == 1]\n",
    "    good_old = prev_points[status == 1]\n",
    "    return good_new.reshape(-1,2), good_old.reshape(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_bounding_box(bbox, transform_matrix):\n",
    "    \n",
    "    transform_matrix = np.vstack([transform_matrix, [0, 0, 1]])\n",
    "\n",
    "    # Extract corners of bounding box\n",
    "    corners = np.array([\n",
    "        [bbox[0], bbox[1]],  # Top-left\n",
    "        [bbox[2], bbox[1]],  # Top-right\n",
    "        [bbox[2], bbox[3]],  # Bottom-right\n",
    "        [bbox[0], bbox[3]]   # Bottom-left\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # Convert corners to homogenous coordinates\n",
    "    ones = np.ones(shape=(len(corners), 1), dtype=np.float32)\n",
    "    corners_homogenous = np.hstack([corners, ones])\n",
    "\n",
    "    # Apply the transformation matrix to the bounding box corners\n",
    "    new_corners_homogenous = np.dot(transform_matrix, corners_homogenous.T).T\n",
    "\n",
    "    # Convert back to standard coordinates\n",
    "    new_corners = new_corners_homogenous[:, :2] / new_corners_homogenous[:, 2][:, None]\n",
    "\n",
    "    # Get the new bounding box as min and max from the transformed corners\n",
    "    x_min, y_min = np.min(new_corners, axis=0)\n",
    "    x_max, y_max = np.max(new_corners, axis=0)\n",
    "    \n",
    "    return [x_min, y_min, x_max, y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_affine_transform(old_points, new_points):\n",
    "    transform_matrix, inliers = cv2.estimateAffine2D(old_points, new_points)\n",
    "    return transform_matrix  # 2x3 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"last.pt\")\n",
    "\n",
    "# Open your video\n",
    "cap = cv2.VideoCapture(r'data\\S03\\c010\\vdo.avi')\n",
    "n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Read the first frame\n",
    "# ret, prev_frame = cap.read()\n",
    "\n",
    "# Detect objects in the first frame\n",
    "# bboxes = detect_objects(model, prev_frame)\n",
    "\n",
    "# Initialize tracking points\n",
    "# prev_points = initialize_tracking_points(prev_frame, bboxes)\n",
    "\n",
    "video = cv2.VideoWriter(f'test.mp4', -1, fps, (width, height), True)\n",
    "# cap.set(cv2.CAP_PROP_POS_FRAMES, n_frames//8 - 30)\n",
    "\n",
    "for i in tqdm.tqdm(range(n_frames//4)):\n",
    "    # Read a new frame\n",
    "    ret, current_frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if i % 8 == 0:\n",
    "        # Detect objects in the current frame\n",
    "        bboxes = detect_objects(model, current_frame)\n",
    "\n",
    "        # Initialize tracking points\n",
    "        current_points = initialize_tracking_points(current_frame, bboxes)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Track the points\n",
    "        current_points, prev_points = track_objects(prev_frame, current_frame, prev_points)\n",
    "\n",
    "        for b,bbox in enumerate(bboxes):\n",
    "\n",
    "            # get points inside the bounding box\n",
    "            current_points_inside = current_points[(current_points[:, 0] > bbox[0]) & (current_points[:, 0] < bbox[2]) & (current_points[:, 1] > bbox[1]) & (current_points[:, 1] < bbox[3])]\n",
    "            prev_points_inside = prev_points[(current_points[:, 0] > bbox[0]) & (current_points[:, 0] < bbox[2]) & (current_points[:, 1] > bbox[1]) & (current_points[:, 1] < bbox[3])]\n",
    "\n",
    "            # dx = np.mean(current_points_inside[:, 0] - prev_points_inside[:, 0])\n",
    "            # dy = np.mean(current_points_inside[:, 1] - prev_points_inside[:, 1])\n",
    "\n",
    "            # bbox[0] += dx\n",
    "            # bbox[1] += dy\n",
    "            # bbox[2] += dx\n",
    "            # bbox[3] += dy\n",
    "                   \n",
    "            #Compute the affine transform from the old points to the new points\n",
    "            if len(current_points_inside) >= 3 and len(prev_points_inside) >= 3:  # Need at least 3 points\n",
    "                transform_matrix = estimate_affine_transform(prev_points_inside, current_points_inside)\n",
    "                \n",
    "                if transform_matrix is not None:\n",
    "                    # Update the bounding boxes using the computed transform\n",
    "                    bboxes[b] = update_bounding_box(bbox, transform_matrix)\n",
    "\n",
    "    # Draw bounding boxes and tracking points\n",
    "    for bbox in bboxes:  # Assuming bboxes is a list of (x, y, w, h)\n",
    "        # round the float values\n",
    "        x1, y1, x2, y2 = np.round(bbox).astype(int)\n",
    "        cv2.rectangle(current_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    for point in current_points:\n",
    "        x, y = map(int, point.ravel())\n",
    "        cv2.circle(current_frame, (x, y), 3, (0, 0, 255), -1)\n",
    "    \n",
    "    # Update previous frame and points for the next iteration\n",
    "    prev_frame = current_frame.copy()\n",
    "    prev_points = current_points.reshape(-1, 1, 2)\n",
    "\n",
    "    # Write the frame into the file 'output.avi'\n",
    "    video.write(current_frame)\n",
    "\n",
    "# Release the video and close windows\n",
    "cap.release()\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voc_iou(pred, gt):\n",
    "    \"\"\"\n",
    "    Calculate IoU between detect box and gt boxes.\n",
    "    :param pred: Predicted bounding box coordinates [x1, y1, x2, y2].\n",
    "    :param gt: Ground truth bounding box coordinates [[x1, y1, x2, y2]].\n",
    "    \"\"\"\n",
    "    # compute overlaps\n",
    "    # intersection\n",
    "    ixmin = np.maximum(gt[0], pred[0])\n",
    "    iymin = np.maximum(gt[1], pred[1])\n",
    "    ixmax = np.minimum(gt[2], pred[2])\n",
    "    iymax = np.minimum(gt[3], pred[3])\n",
    "    iw = np.maximum(ixmax - ixmin + 1.0, 0.0)\n",
    "    ih = np.maximum(iymax - iymin + 1.0, 0.0)\n",
    "    inters = iw * ih\n",
    "\n",
    "    # union\n",
    "    uni = (\n",
    "        (pred[2] - pred[0] + 1.0) * (pred[3] - pred[1] + 1.0)\n",
    "        + (gt[2] - gt[0] + 1.0) * (gt[3] - gt[1] + 1.0)\n",
    "        - inters\n",
    "    )\n",
    "\n",
    "    return inters / uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 267/267 [01:21<00:00,  3.28it/s]\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"last.pt\")\n",
    "start_frame = 0\n",
    "cap = cv2.VideoCapture(r'data\\S03\\c010\\vdo.avi')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "detections = [detect_objects(model, cap.read()[1]) for _ in tqdm.tqdm(range(start_frame, n_frames//8))]\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/266 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [09:13<00:00,  2.08s/it]\n",
      "100%|██████████| 266/266 [08:35<00:00,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "def get_flow_region(prev_roi, current_roi):\n",
    "\n",
    "    # Convert frames to grayscale\n",
    "    prev_gray = cv2.cvtColor(prev_roi, cv2.COLOR_BGR2GRAY)\n",
    "    current_gray = cv2.cvtColor(current_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect Harris corners in the ROI\n",
    "    # prev_points = cv2.goodFeaturesToTrack(prev_gray, maxCorners=100, qualityLevel=0.2, minDistance=3)\n",
    "    # if prev_points is None:\n",
    "    #     return np.array([])\n",
    "\n",
    "    h, w = prev_gray.shape\n",
    "    y, x = np.mgrid[0:h, 0:w].astype(np.float32)\n",
    "    prev_points = np.stack((x, y), axis=-1).reshape(-1, 1, 2)\n",
    "\n",
    "    # Calculate optical flow (Lucas-Kanade)\n",
    "    current_points, status, error = cv2.calcOpticalFlowPyrLK(prev_gray, current_gray, prev_points, None)\n",
    "    # Select good points (where status == 1)\n",
    "    good_new = current_points[status == 1]\n",
    "    good_old = prev_points[status == 1]\n",
    "    # Compute the flow vectors\n",
    "    flow_vectors = good_new - good_old\n",
    "    return flow_vectors\n",
    "    \n",
    "\n",
    "def track_objects(detections, start_frame=0, optical_flow=True):\n",
    "    \"\"\"\n",
    "    Tracking of objects across frames using IoU and optical flow.\n",
    "    detections: list of lists containing detected bounding boxes for each frame.\n",
    "    \"\"\"\n",
    "    active_objects = {}  # Maps object ID to last seen bounding box\n",
    "    next_track_id = 0\n",
    "    iou_threshold = 0.3  # Minimum IoU to consider a match\n",
    "\n",
    "    tracking = dict()\n",
    "    tracking_video = []\n",
    "    cap = cv2.VideoCapture(r'data\\S03\\c010\\vdo.avi')\n",
    "    _, prev_frame = cap.read()\n",
    "\n",
    "    # Initialize tracks with the first frame detections\n",
    "    for box in detections[0]:\n",
    "        active_objects[next_track_id] = box\n",
    "        next_track_id += 1\n",
    "\n",
    "    tracking_video.append(active_objects)\n",
    "    tracking[1] = [value + [key] for key, value in active_objects.items()]\n",
    "\n",
    "    # Iterate over each frame\n",
    "    for idx, current_detections in tqdm.tqdm(enumerate(detections[1:], start=1), total=len(detections[1:])):\n",
    "        _, current_frame = cap.read()\n",
    "\n",
    "        updated_tracks = {}\n",
    "        for track_id, box in active_objects.items():\n",
    "            # Estimate new position using optical flow (or any other means)\n",
    "            flow_region = get_flow_region(prev_frame[box[1]:box[3], box[0]:box[2]], current_frame[box[1]:box[3], box[0]:box[2]])\n",
    "            if flow_region.size > 0 and optical_flow:\n",
    "                dx, dy = np.max(flow_region, axis=0)\n",
    "                new_box = np.round([box[0] + dx, box[1] + dy, box[2] + dx, box[3] + dy]).astype(int)\n",
    "            else:\n",
    "                new_box = box  # No flow information, keep the old box\n",
    "\n",
    "            updated_tracks[track_id] = new_box  # Update with new position\n",
    "\n",
    "        # Match current detections to updated tracks based on IoU\n",
    "        current_objects = {}\n",
    "        for bbox_curr in current_detections:\n",
    "            best_id, max_iou = None, 0\n",
    "            for track_id, bbox_prev in updated_tracks.items():\n",
    "                iou = voc_iou(bbox_curr, bbox_prev)\n",
    "                if iou > max_iou:\n",
    "                    max_iou, best_id = iou, track_id\n",
    "\n",
    "            if max_iou > iou_threshold:\n",
    "                current_objects[best_id] = bbox_curr\n",
    "            else:\n",
    "                current_objects[next_track_id] = bbox_curr\n",
    "                next_track_id += 1\n",
    "\n",
    "        # Update tracking information for the next frame\n",
    "        active_objects = current_objects\n",
    "        tracking_video.append(active_objects)\n",
    "        tracking[idx+start_frame+1] = [value + [key] for key, value in active_objects.items()]\n",
    "\n",
    "        prev_frame = current_frame.copy()  # Update the frame for the next iteration\n",
    "\n",
    "    return tracking, tracking_video # Return the tracking information\n",
    "\n",
    "tracking, tracking_video = track_objects(detections, start_frame, optical_flow=False)\n",
    "tracking_of, tracking_video_of = track_objects(detections, start_frame, optical_flow=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:13<00:00,  4.43s/it]\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture('data/S03/c010/vdo.avi')\n",
    "\n",
    "# Store tracking history for each object\n",
    "tracking_history = {}\n",
    "tracking_history_of = {}\n",
    "# Store colors for each object ID\n",
    "colors = {}\n",
    "\n",
    "for start in tqdm.tqdm(range(start_frame, n_frames//8, 100)):\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "    video = cv2.VideoWriter(f'tracking/yolotrackerof/tracking_{start}.mp4', -1, fps, (width, height), True)\n",
    "\n",
    "    for i in range(start, min(start + 100, n_frames//8)):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Draw detected bounding boxes and tracking lines\n",
    "        for obj_id, bbox in tracking_video[i-start_frame].items():\n",
    "            # Assign a unique color if new object\n",
    "            if obj_id not in colors:\n",
    "                colors[obj_id] = (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255))\n",
    "\n",
    "            # Draw the bounding box\n",
    "            start_point = (int(bbox[0]), int(bbox[1]))\n",
    "            end_point = (int(bbox[2]), int(bbox[3]))\n",
    "            frame = cv2.rectangle(frame, start_point, end_point, (0,255,0), 1)\n",
    "            frame = cv2.putText(frame, str(obj_id), start_point, cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 1, cv2.LINE_AA)\n",
    "            \n",
    "            # Update tracking history\n",
    "            center_position = ((start_point[0] + end_point[0]) // 2, (start_point[1] + end_point[1]) // 2)\n",
    "            if obj_id not in tracking_history:\n",
    "                tracking_history[obj_id] = [center_position]\n",
    "            else:\n",
    "                tracking_history[obj_id].append(center_position)\n",
    "            \n",
    "            # Draw tracking line (polyline for all historical positions)\n",
    "            if len(tracking_history[obj_id]) > 1:\n",
    "                for j in range(1, len(tracking_history[obj_id])):\n",
    "                    cv2.line(frame, tracking_history[obj_id][j - 1], tracking_history[obj_id][j], (0,255,0), 2)\n",
    "\n",
    "        # Draw detected bounding boxes and tracking lines\n",
    "        for obj_id, bbox in tracking_video_of[i-start_frame].items():\n",
    "            # Assign a unique color if new object\n",
    "            if obj_id not in colors:\n",
    "                colors[obj_id] = (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255))\n",
    "\n",
    "            # Draw the bounding box\n",
    "            start_point = (int(bbox[0]), int(bbox[1]))\n",
    "            end_point = (int(bbox[2]), int(bbox[3]))\n",
    "            frame = cv2.rectangle(frame, start_point, end_point, (255,0,0), 1)\n",
    "            frame = cv2.putText(frame, str(obj_id), start_point, cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "            \n",
    "            # Update tracking history\n",
    "            center_position = ((start_point[0] + end_point[0]) // 2, (start_point[1] + end_point[1]) // 2)\n",
    "            if obj_id not in tracking_history_of:\n",
    "                tracking_history_of[obj_id] = [center_position]\n",
    "            else:\n",
    "                tracking_history_of[obj_id].append(center_position)\n",
    "            \n",
    "            # Draw tracking line (polyline for all historical positions)\n",
    "            if len(tracking_history_of[obj_id]) > 1:\n",
    "                for j in range(1, len(tracking_history_of[obj_id])):\n",
    "                    cv2.line(frame, tracking_history_of[obj_id][j - 1], tracking_history_of[obj_id][j], (255,0,0), 2)\n",
    "\n",
    "        video.write(frame)\n",
    "\n",
    "video.release()\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
