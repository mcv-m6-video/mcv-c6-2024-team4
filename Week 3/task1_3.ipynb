{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pyflow.pyflow as pyflow\n",
    "import xml.etree.ElementTree as elemTree\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(model, image):\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        results = model(image, verbose=False)\n",
    "    # Extract bounding box coordinates, labels, and confidence scores\n",
    "    return [box.xyxy[0].to(int).tolist() for box in results[0].boxes if box.cls == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voc_iou(pred, gt):\n",
    "    \"\"\"\n",
    "    Calculate IoU between detect box and gt boxes.\n",
    "    :param pred: Predicted bounding box coordinates [x1, y1, x2, y2].\n",
    "    :param gt: Ground truth bounding box coordinates [[x1, y1, x2, y2]].\n",
    "    \"\"\"\n",
    "    # compute overlaps\n",
    "    # intersection\n",
    "    ixmin = np.maximum(gt[0], pred[0])\n",
    "    iymin = np.maximum(gt[1], pred[1])\n",
    "    ixmax = np.minimum(gt[2], pred[2])\n",
    "    iymax = np.minimum(gt[3], pred[3])\n",
    "    iw = np.maximum(ixmax - ixmin + 1.0, 0.0)\n",
    "    ih = np.maximum(iymax - iymin + 1.0, 0.0)\n",
    "    inters = iw * ih\n",
    "\n",
    "    # union\n",
    "    uni = (\n",
    "        (pred[2] - pred[0] + 1.0) * (pred[3] - pred[1] + 1.0)\n",
    "        + (gt[2] - gt[0] + 1.0) * (gt[3] - gt[1] + 1.0)\n",
    "        - inters\n",
    "    )\n",
    "\n",
    "    return inters / uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8m.pt\")\n",
    "start_frame = 0\n",
    "cap = cv2.VideoCapture(r'data\\S03\\c010\\vdo.avi')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "detections = [detect_objects(model, cap.read()[1]) for _ in tqdm.tqdm(range(start_frame, n_frames))]\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flow_region(prev_roi, current_roi):\n",
    "\n",
    "    if prev_roi.shape[0] == 0 or prev_roi.shape[1] == 0 or current_roi.shape[0] == 0 or current_roi.shape[1] == 0:\n",
    "        return 0,0\n",
    "    \n",
    "    prev_roi = cv2.resize(prev_roi, (64, 64)).astype(float)  / 255\n",
    "    current_roi = cv2.resize(current_roi, (64, 64)).astype(float)  / 255\n",
    "\n",
    "    u, v, _ = pyflow.coarse2fine_flow(prev_roi, current_roi, 0.01, 0.5, 30, 7, 1, 30)\n",
    "\n",
    "    dx = np.max(u)\n",
    "    dy = np.max(v)\n",
    "\n",
    "    return dx, dy\n",
    "\n",
    "\n",
    "def track_objects(detections, start_frame=0, optical_flow=True):\n",
    "    \"\"\"\n",
    "    Tracking of objects across frames using IoU and optical flow.\n",
    "    detections: list of lists containing detected bounding boxes for each frame.\n",
    "    \"\"\"\n",
    "    active_objects = {}  # Maps object ID to last seen bounding box\n",
    "    next_track_id = 0\n",
    "    iou_threshold = 0.3  # Minimum IoU to consider a match\n",
    "\n",
    "    tracking = dict()\n",
    "    tracking_video = []\n",
    "    cap = cv2.VideoCapture(r'data\\S03\\c010\\vdo.avi')\n",
    "    _, prev_frame = cap.read()\n",
    "\n",
    "    # Initialize tracks with the first frame detections\n",
    "    for box in detections[0]:\n",
    "        active_objects[next_track_id] = box\n",
    "        next_track_id += 1\n",
    "\n",
    "    tracking_video.append(active_objects)\n",
    "    tracking[1] = [value + [key] for key, value in active_objects.items()]\n",
    "\n",
    "    # Iterate over each frame\n",
    "    for idx, current_detections in tqdm.tqdm(enumerate(detections[1:], start=1), total=len(detections[1:])):\n",
    "        _, current_frame = cap.read()\n",
    "\n",
    "        if optical_flow:\n",
    "            flows = {}\n",
    "            for track_id, box in active_objects.items():\n",
    "                # Estimate new position using optical flow (or any other means)\n",
    "                dx, dy = get_flow_region(prev_frame[box[1]:box[3], box[0]:box[2]], current_frame[box[1]:box[3], box[0]:box[2]])\n",
    "                flows[track_id] = [dx, dy]\n",
    "\n",
    "        # Match current detections to updated tracks based on IoU\n",
    "        current_objects = {}\n",
    "        for bbox_curr in current_detections:\n",
    "            best_id, max_iou = None, 0\n",
    "                \n",
    "            for track_id, bbox_prev in active_objects.items():\n",
    "                if optical_flow:\n",
    "                    bbox_curr = np.round([bbox_curr[0] + flows[track_id][0], bbox_curr[1] + flows[track_id][1], bbox_curr[2] + flows[track_id][0], bbox_curr[3] + flows[track_id][1]]).astype(int).tolist()\n",
    "                iou = voc_iou(bbox_curr, bbox_prev)\n",
    "                if iou > max_iou:\n",
    "                    max_iou, best_id = iou, track_id\n",
    "\n",
    "            if max_iou > iou_threshold:\n",
    "                current_objects[best_id] = bbox_curr\n",
    "            else:\n",
    "                current_objects[next_track_id] = bbox_curr\n",
    "                next_track_id += 1\n",
    "\n",
    "        # Update tracking information for the next frame\n",
    "        active_objects = current_objects\n",
    "        tracking_video.append(active_objects)\n",
    "        tracking[idx+start_frame+1] = [value + [key] for key, value in active_objects.items()]\n",
    "\n",
    "        prev_frame = current_frame.copy()  # Update the frame for the next iteration\n",
    "\n",
    "    return tracking, tracking_video # Return the tracking information\n",
    "\n",
    "tracking, tracking_video = track_objects(detections, start_frame, optical_flow=False)\n",
    "tracking_of, tracking_video_of = track_objects(detections, start_frame, optical_flow=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('data/S03/c010/vdo.avi')\n",
    "\n",
    "# Store tracking history for each object\n",
    "tracking_history = {}\n",
    "tracking_history_of = {}\n",
    "# Store colors for each object ID\n",
    "colors = {}\n",
    "\n",
    "for start in tqdm.tqdm(range(start_frame, n_frames, 100)):\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "    video = cv2.VideoWriter(f'tracking/trackerof_joint/tracking_{start}.mp4', -1, fps, (width, height), True)\n",
    "\n",
    "    for i in range(start, min(start + 100, n_frames)):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        for obj_id, bbox in tracking_video[i-start_frame].items():\n",
    "            # Assign a unique color if new object\n",
    "            if obj_id not in colors:\n",
    "                colors[obj_id] = (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255))\n",
    "\n",
    "            # Draw the bounding box\n",
    "            start_point = (int(bbox[0]), int(bbox[1]))\n",
    "            end_point = (int(bbox[2]), int(bbox[3]))\n",
    "            frame = cv2.rectangle(frame, start_point, end_point, (0,0,255), 2)\n",
    "            # frame = cv2.rectangle(frame, start_point, end_point, colors[obj_id], 2)\n",
    "            frame = cv2.putText(frame, str(obj_id), start_point, cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2, cv2.LINE_AA)\n",
    "            # frame = cv2.putText(frame, str(obj_id), start_point, cv2.FONT_HERSHEY_SIMPLEX, 1, colors[obj_id], 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Update tracking history\n",
    "            center_position = ((start_point[0] + end_point[0]) // 2, (start_point[1] + end_point[1]) // 2)\n",
    "            if obj_id not in tracking_history:\n",
    "                tracking_history[obj_id] = [center_position]\n",
    "            else:\n",
    "                tracking_history[obj_id].append(center_position)\n",
    "            \n",
    "            # Draw tracking line (polyline for all historical positions)\n",
    "            if len(tracking_history[obj_id]) > 1:\n",
    "                for j in range(1, len(tracking_history[obj_id])):\n",
    "                    cv2.line(frame, tracking_history[obj_id][j - 1], tracking_history[obj_id][j], (0,0,255), 2)\n",
    "                    # cv2.line(frame, tracking_history[obj_id][j - 1], tracking_history[obj_id][j], colors[obj_id], 2)\n",
    "\n",
    "        # Draw detected bounding boxes and tracking lines\n",
    "        for obj_id, bbox in tracking_video_of[i-start_frame].items():\n",
    "            # Assign a unique color if new object\n",
    "            if obj_id not in colors:\n",
    "                colors[obj_id] = (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255))\n",
    "\n",
    "            # Draw the bounding box\n",
    "            start_point = (int(bbox[0]), int(bbox[1]))\n",
    "            end_point = (int(bbox[2]), int(bbox[3]))\n",
    "            frame = cv2.rectangle(frame, start_point, end_point, (255,0,0), 2)\n",
    "            # frame = cv2.rectangle(frame, start_point, end_point, colors[obj_id], 2)\n",
    "            frame = cv2.putText(frame, str(obj_id), start_point, cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "            # frame = cv2.putText(frame, str(obj_id), start_point, cv2.FONT_HERSHEY_SIMPLEX, 1, colors[obj_id], 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Update tracking history\n",
    "            center_position = ((start_point[0] + end_point[0]) // 2, (start_point[1] + end_point[1]) // 2)\n",
    "            if obj_id not in tracking_history_of:\n",
    "                tracking_history_of[obj_id] = [center_position]\n",
    "            else:\n",
    "                tracking_history_of[obj_id].append(center_position)\n",
    "            \n",
    "            # Draw tracking line (polyline for all historical positions)\n",
    "            if len(tracking_history_of[obj_id]) > 1:\n",
    "                for j in range(1, len(tracking_history_of[obj_id])):\n",
    "                    # cv2.line(frame, tracking_history_of[obj_id][j - 1], tracking_history_of[obj_id][j], colors[obj_id], 2)\n",
    "                    cv2.line(frame, tracking_history_of[obj_id][j - 1], tracking_history_of[obj_id][j], (255,0,0), 2)\n",
    "\n",
    "        video.write(frame)\n",
    "video.release()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder values for <conf>, <x>, <y>, <z> since these are not provided\n",
    "conf, x, y, z = 1, -1, -1, -1  # Using -1 to indicate unknown or not applicable\n",
    "\n",
    "# Convert data to the required gt.txt format\n",
    "gt_content = []\n",
    "for frame, bboxes in tracking_of.items():\n",
    "    for bbox in bboxes:\n",
    "        bb_left, bb_top, bb_right, bb_bottom, obj_id = map(int, bbox)\n",
    "        bb_width = bb_right - bb_left\n",
    "        bb_height = bb_bottom - bb_top\n",
    "        gt_content.append(f\"{frame}, {obj_id}, {bb_left}, {bb_top}, {bb_width}, {bb_height}, {conf}, {x}, {y}, {z}\")\n",
    "\n",
    "# Join all entries to form the final content for the gt.txt file\n",
    "gt_text = \"\\n\".join(gt_content)\n",
    "\n",
    "file_path = 'TrackEval/data/trackers/mot_challenge/week3-train/yolotracker/data/week3-01.txt'  # Define the file path\n",
    "with open(file_path, 'w') as f:\n",
    "    f.write(gt_text)\n",
    "\n",
    "!python TrackEval/scripts/run_mot_challenge.py --BENCHMARK week3 --SPLIT_TO_EVAL train --TRACKERS_TO_EVAL yolotracker --METRICS HOTA Identity --DO_PREPROC False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
