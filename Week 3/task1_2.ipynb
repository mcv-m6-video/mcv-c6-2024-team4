{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pyflow.pyflow as pyflow\n",
    "import numpy as np\n",
    "from evaluation import MSEN, PEPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code imported from https://github.com/mcv-m6-video/mcv-m6-2023-team2/blob/main/week4/utils_w4.py\n",
    "def convert_optical_flow_to_image(flow):\n",
    "    # The 3-channel uint16 PNG images that comprise optical flow maps contain information\n",
    "    # on the u-component in the first channel, the v-component in the second channel,\n",
    "    # and whether a valid ground truth optical flow value exists for a given pixel in the third channel.\n",
    "    # A value of 1 in the third channel indicates the existence of a valid optical flow value\n",
    "    # while a value of 0 indicates otherwise. To convert the u- and v-flow values from\n",
    "    # their original uint16 format to floating point values, one can do so by subtracting 2^15 from the value,\n",
    "    # converting it to float, and then dividing the result by 64.\n",
    "\n",
    "    img_u = (flow[:, :, 2] - 2 ** 15) / 64\n",
    "    img_v = (flow[:, :, 1] - 2 ** 15) / 64\n",
    "\n",
    "    assert (flow[:, :, 0] > 1).sum() == 0 # all values are 0 or 1\n",
    "\n",
    "    img_u[flow[:, :, 0] == 0] = 0\n",
    "    img_v[flow[:, :, 0] == 0] = 0\n",
    "\n",
    "    optical_flow = np.dstack((img_u, img_v, flow[:, :, 0]))\n",
    "    return optical_flow\n",
    "\n",
    "gt_flow = cv2.imread(r'kitti_data\\training\\flow_noc\\000045_10.png', cv2.IMREAD_UNCHANGED).astype(np.double)\n",
    "gt_flow_img = convert_optical_flow_to_image(gt_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSEN: 10.608176924923452\n",
      "PEPN: 78.47598964823158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1100x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1100x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "first = cv2.imread(r'kitti_data\\training\\image_0\\000045_10.png').astype(float)  / 255\n",
    "second = cv2.imread(r'kitti_data\\training\\image_0\\000045_11.png').astype(float) / 255\n",
    "\n",
    "first.shape, second.shape, gt_flow.shape\n",
    "\n",
    "u, v, im2W = pyflow.coarse2fine_flow(first, second)\n",
    "\n",
    "pred_flow = np.stack((u, v, np.ones_like(u)), axis=2)\n",
    "\n",
    "msen, sen = MSEN(gt_flow_img, pred_flow, './results')\n",
    "pepn = PEPN(sen)\n",
    "\n",
    "print('MSEN:', msen)\n",
    "print('PEPN:', pepn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = cv2.imread(r'kitti_data\\training\\image_0\\000045_10.png')\n",
    "second = cv2.imread(r'kitti_data\\training\\image_0\\000045_11.png')\n",
    "first_gray = cv2.cvtColor(first, cv2.COLOR_BGR2GRAY)\n",
    "second_gray = cv2.cvtColor(second, cv2.COLOR_BGR2GRAY)\n",
    "# Assuming prev_gray is your previous grayscale image\n",
    "h, w = first_gray.shape\n",
    "\n",
    "# Create a grid of points\n",
    "y, x = np.mgrid[0:h, 0:w].astype(np.float32)\n",
    "points = np.stack((x, y), axis=-1).reshape(-1, 1, 2)\n",
    "\n",
    "# Now you can use cv2.calcOpticalFlowPyrLK() with all pixels\n",
    "new_points, status, error = cv2.calcOpticalFlowPyrLK(first_gray, second_gray, points, None)\n",
    "\n",
    "# Create a mask to select good points\n",
    "new_points[status == 0] = 0\n",
    "points[status == 0] = 0\n",
    "\n",
    "flow = new_points - points\n",
    "\n",
    "u = flow[:, 0, 0].reshape(h, w)\n",
    "v = flow[:, 0, 1].reshape(h, w)\n",
    "\n",
    "pred_flow = np.stack((u, v, np.ones_like(u)), axis=2)\n",
    "\n",
    "msen, sen = MSEN(gt_flow_img, pred_flow, './results2')\n",
    "pepn = PEPN(sen)\n",
    "\n",
    "print('MSEN:', msen)\n",
    "print('PEPN:', pepn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Luis\\miniconda3\\lib\\site-packages\\torch\\functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from core.raft import RAFT\n",
    "from core.utils.utils import InputPadder\n",
    "import torch\n",
    "\n",
    "def load_image(img):\n",
    "    img = np.array(img).astype(np.uint8)\n",
    "    img = torch.from_numpy(img).permute(2, 0, 1).float()\n",
    "    return img[None]\n",
    "\n",
    "import argparse\n",
    "\n",
    "# Define a function to create and parse arguments\n",
    "def create_args(small):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--small', action='store_true', help='use small model', default=small)\n",
    "    parser.add_argument('--mixed_precision', action='store_true', help='use mixed precision')\n",
    "    args, unknown = parser.parse_known_args()  # This line parses the arguments\n",
    "    return args\n",
    "\n",
    "# Create an 'args' variable with default values or specified values.\n",
    "args = create_args(small=False)\n",
    "\n",
    "first = cv2.imread(r'kitti_data\\training\\image_0\\000045_10.png')\n",
    "second = cv2.imread(r'kitti_data\\training\\image_0\\000045_11.png')\n",
    "\n",
    "shape = first.shape\n",
    "\n",
    "image1 = load_image(first)\n",
    "image2 = load_image(second)\n",
    "\n",
    "model = torch.nn.DataParallel(RAFT(args))\n",
    "model.load_state_dict(torch.load('raft-kitti.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "padder = InputPadder(image1.shape)\n",
    "image1, image2 = padder.pad(image1, image2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    flow_low, flow_up = model(image1, image2, iters=20, test_mode=True)\n",
    "\n",
    "# Resize flow_up to original image1 size\n",
    "flow_up = torch.nn.functional.interpolate(flow_up, size=shape[:2], mode='bilinear', align_corners=True)\n",
    "flow_up = flow_up.numpy().squeeze().transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSEN: 0.5591758013461698\n",
      "PEPN: 1.3773602990510878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1100x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1100x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_flow = np.stack((flow_up[:, :, 0], flow_up[:, :, 1], np.ones_like(flow_up[:, :, 0])), axis=2)\n",
    "msen, sen = MSEN(gt_flow_img, pred_flow, './results2')\n",
    "pepn = PEPN(sen)\n",
    "\n",
    "print('MSEN:', msen)\n",
    "print('PEPN:', pepn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
