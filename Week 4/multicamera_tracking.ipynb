{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "from sort import Sort\n",
    "import os, copy\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition remains the same\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, sequence):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights='ResNet50_Weights.DEFAULT')\n",
    "        self.model.fc = nn.Identity()\n",
    "        self.sequence = sequence\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def get_embedding(self, bbox, frame_num, camera_id):\n",
    "\n",
    "        name = 'vdo10.avi' if camera_id == 'c015' else 'vdo.avi'\n",
    "        path = f\"aic19-track1-mtmc-train/train/{self.sequence}/{camera_id}/{name}\"\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        frame = frame[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = torch.tensor((cv2.resize(frame, (224, 224)) / 255.0).transpose(2, 0, 1), dtype=torch.float32).unsqueeze(0).cuda()\n",
    "        return self.model(frame)[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths = ['aic19-track1-mtmc-train/train/S01/c001/vdo.avi', \n",
    "               'aic19-track1-mtmc-train/train/S01/c002/vdo.avi', \n",
    "               'aic19-track1-mtmc-train/train/S01/c003/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S01/c004/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S01/c005/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S03/c010/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S03/c011/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S03/c012/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S03/c013/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S03/c014/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S03/c015/vdo10.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c016/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c017/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c018/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c019/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c020/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c021/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c022/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c023/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c024/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c025/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c026/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c027/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c028/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c029/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c030/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c031/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c032/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c033/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c034/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c035/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c036/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c037/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c038/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c039/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c040/vdo.avi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upscale_fps(input_path, output_path, input_fps=8, target_fps=10):\n",
    "    \"\"\"\n",
    "    This function upscales the FPS of a video by interpolating frames using OpenCV and NumPy.\n",
    "    \n",
    "    Parameters:\n",
    "    input_path (str): The path to the input video file.\n",
    "    output_path (str): The path where the output video will be saved.\n",
    "    input_fps (int): The original FPS of the video.\n",
    "    target_fps (int): The target FPS to upscale the video to.\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    \n",
    "    # Capture video from input path\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        return \"Failed to open video file.\"\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, target_fps, (width, height))\n",
    "    \n",
    "    prev_frame = None\n",
    "    success, frame = cap.read()\n",
    "    while success:\n",
    "        out.write(frame)  # Write the original frame\n",
    "        \n",
    "        # If there's a previous frame, generate interpolated frames\n",
    "        if prev_frame is not None:\n",
    "            # Generate interpolated frames\n",
    "            for _ in range((target_fps // input_fps) - 1):\n",
    "                interp_frame = cv2.addWeighted(prev_frame, 0.5, frame, 0.5, 0)\n",
    "                out.write(interp_frame)\n",
    "                \n",
    "        prev_frame = frame\n",
    "        success, frame = cap.read()\n",
    "        \n",
    "    # Release everything if job is finished\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    return \"Video FPS upscaled successfully.\"\n",
    "\n",
    "# upscale_fps('aic19-track1-mtmc-train/train/S03/c015/vdo.avi', 'aic19-track1-mtmc-train/train/S03/c015/vdo10.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_detections(video_path, yolo):\n",
    "    \"\"\"\n",
    "    Given a video, create detections for each frame in the video and store bounding boxes in a text file\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Create a text file to store the bounding boxes\n",
    "    f = open(f\"detections/{video_path.split('/')[-2].split('.')[0]}.txt\", \"w\")\n",
    "\n",
    "    frame_num = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Get the detections for the frame\n",
    "        detections = yolo(frame, verbose=False)\n",
    "\n",
    "        # Write the bounding boxes to the text file\n",
    "        for box in detections[0].boxes:\n",
    "            if box.cls == 2:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].int().tolist()\n",
    "                f.write(f\"{frame_num},{x1},{y1},{x2},{y2}\\n\")\n",
    "\n",
    "        frame_num += 1\n",
    "\n",
    "    f.close()\n",
    "    cap.release()\n",
    "\n",
    "# for video in tqdm(video_paths):\n",
    "#     create_detections(video, yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_detections_to_sort_format(detections):\n",
    "    \"\"\"\n",
    "    Convert detections to the format expected by the SORT algorithm. Stacks all the detections of the same frame into a single array.\n",
    "    \"\"\"\n",
    "    max_frame = detections[-1][0]\n",
    "    all_detections = []\n",
    "    for frame_num in range(max_frame + 1):\n",
    "        frame_detections = [d[1:] + [1] for d in detections if d[0] == frame_num]\n",
    "        if len(frame_detections) == 0:\n",
    "            # print(f\"No detections found for frame {frame_num}\")\n",
    "            frame_detections = np.empty((0, 5))\n",
    "        all_detections.append(frame_detections)\n",
    "    return all_detections\n",
    "\n",
    "def compute_similarity(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean distance between two embeddings.\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(embedding1 - embedding2)\n",
    "\n",
    "sequence = 'S01'\n",
    "if sequence == 'S01':\n",
    "    camera_ids = ['c001', 'c002', 'c003', 'c004', 'c005']\n",
    "elif sequence == 'S03':\n",
    "    camera_ids = ['c010', 'c011', 'c012', 'c013', 'c014', 'c015']\n",
    "elif sequence == 'S04':\n",
    "    camera_ids = ['c016', 'c017', 'c018', 'c019', 'c020', 'c021', 'c022', 'c023', 'c024', 'c025', 'c026', 'c027', 'c028', 'c029', 'c030', 'c031', 'c032', 'c033', 'c034', 'c035', 'c036', 'c037', 'c038', 'c039', 'c040']\n",
    "\n",
    "siamese_net = Net(sequence).cuda().eval()\n",
    "siamese_net.load_state_dict(torch.load('siamese_model_6.pth', map_location=torch.device('cuda')))\n",
    "# yolo = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Load detections from text files\n",
    "detections = {}\n",
    "for camera_id in camera_ids:\n",
    "    with open(f'detections/{camera_id}.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        detections[camera_id] = [[int(x) for x in line.strip().split(',')] for line in lines]\n",
    "\n",
    "# Compute single trackings for each camera\n",
    "trackings = {c_id: [] for c_id in camera_ids}\n",
    "for camera_id, dets in tqdm(detections.items(), desc=\"Computing trackings\"):\n",
    "    dets_sort = convert_detections_to_sort_format(dets)#[:150]\n",
    "    mot_tracker = Sort()\n",
    "    for frame_num, frame_dets in enumerate(dets_sort):\n",
    "        d = mot_tracker.update(np.array(frame_dets)).astype(int)\n",
    "        d = [[max(0, x) for x in det] for det in d]\n",
    "        trackings[camera_id].append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pre-compute all embeddings for each detection in every camera and frame\n",
    "# precomputed_embeddings = {}\n",
    "# for cam_id in camera_ids:\n",
    "#     precomputed_embeddings[cam_id] = {}\n",
    "#     for frame, detections in tqdm(enumerate(trackings[cam_id]), desc=f\"Precomputing embeddings for {cam_id}\", total=len(trackings[cam_id])):\n",
    "#         with torch.no_grad():\n",
    "#             precomputed_embeddings[cam_id][frame] = [\n",
    "#                 siamese_net.get_embedding(det[:4], frame, cam_id) for det in detections\n",
    "#             ]\n",
    "\n",
    "# with open('precomputed_embeddings.pkl', 'wb') as f:\n",
    "#     pickle.dump(precomputed_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('precomputed_embeddings_s01.pkl', 'rb') as f:\n",
    "    precomputed_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating global trackings for c001 and c005: 100%|██████████| 1403/1403 [00:00<00:00, 3620.68it/s]\n",
      "Updating global trackings for c001 and c004: 100%|██████████| 1293/1293 [00:00<00:00, 4642.36it/s]\n",
      "Updating global trackings for c001 and c002: 100%|██████████| 2117/2117 [00:00<00:00, 5272.81it/s]\n",
      "Updating global trackings for c001 and c003: 100%|██████████| 2202/2202 [00:00<00:00, 4868.13it/s]\n",
      "Updating global trackings for c002 and c004: 100%|██████████| 3232/3232 [00:00<00:00, 4816.54it/s]\n",
      "Updating global trackings for c002 and c003: 100%|██████████| 3196/3196 [00:00<00:00, 5284.43it/s]\n",
      "Updating global trackings for c002 and c005: 100%|██████████| 4178/4178 [00:00<00:00, 5112.61it/s]\n",
      "Updating global trackings for c003 and c004: 100%|██████████| 4643/4643 [00:00<00:00, 4966.29it/s]\n",
      "Updating global trackings for c003 and c005: 100%|██████████| 3487/3487 [00:00<00:00, 5249.06it/s]\n",
      "Updating global trackings for c004 and c005: 100%|██████████| 6384/6384 [00:01<00:00, 5254.91it/s]\n"
     ]
    }
   ],
   "source": [
    "matches = {}\n",
    "# Initialize a dictionary to track the best match for each detection across all cameras and frames\n",
    "best_matches = {cam_id: {frame: {} for frame in range(len(trackings[cam_id]))} for cam_id in camera_ids}\n",
    "\n",
    "for i, cam1 in enumerate(camera_ids):\n",
    "    for j, cam2 in enumerate(camera_ids[i+1:]):\n",
    "        if cam1 == cam2:\n",
    "            continue  # Skip comparing the camera with itself\n",
    "\n",
    "        for frame1, detections1 in enumerate(trackings[cam1]):\n",
    "            for frame2, detections2 in enumerate(trackings[cam2]):\n",
    "                if abs(frame1 - frame2) > 70:\n",
    "                    if frame1 < frame2:\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                for idx1, det1 in enumerate(detections1):\n",
    "                    for idx2, det2 in enumerate(detections2):\n",
    "                        embedding1 = precomputed_embeddings[cam1][frame1][idx1]\n",
    "                        embedding2 = precomputed_embeddings[cam2][frame2][idx2]\n",
    "                        similarity = compute_similarity(embedding1, embedding2)\n",
    "\n",
    "                        current_best = best_matches[cam1][frame1].get(idx1, (None, None, float('inf'), None, None))\n",
    "                        _, _, current_best_similarity, _, _ = current_best\n",
    "\n",
    "                        if similarity < current_best_similarity:\n",
    "                            best_matches[cam1][frame1][idx1] = (cam2, frame2, similarity, idx2, det2)\n",
    "\n",
    "for cam1, frames in best_matches.items():\n",
    "    for frame1, detections in frames.items():\n",
    "        for idx1, (cam2, frame2, _, idx2, det2) in detections.items():\n",
    "            if cam2 is None:\n",
    "                continue  # No match found for this detection\n",
    "            if (cam1, cam2) not in matches:\n",
    "                matches[(cam1, cam2)] = []\n",
    "            det1 = trackings[cam1][frame1][idx1]\n",
    "            matches[(cam1, cam2)].append((frame1, frame2, det1, det2))\n",
    "\n",
    "trackings_global = copy.deepcopy(trackings)\n",
    "for (cam1, cam2), match_list in matches.items():\n",
    "    for frame1, frame2, det1, det2 in tqdm(match_list, desc=f\"Updating global trackings for {cam1} and {cam2}\"):\n",
    "\n",
    "        for frame_num, bbox_local in enumerate(trackings[cam2]):\n",
    "            if frame2 == frame_num:\n",
    "                for idx,bbox in enumerate(trackings[cam2][frame2]):\n",
    "                    if bbox[-1] == det2[-1]:\n",
    "                        break\n",
    "\n",
    "                for idx2,bbox in enumerate(trackings[cam1][frame1]):\n",
    "                    if bbox[-1] == det1[-1]:\n",
    "                        break\n",
    "                \n",
    "                # print(cam1, frame1, idx2, cam2, frame2, idx)\n",
    "                trackings_global[cam1][frame1][idx2][-1] = trackings[cam2][frame2][idx][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 200/200 [00:03<00:00, 65.10it/s]\n",
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 200/200 [00:03<00:00, 57.52it/s]\n",
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 200/200 [00:03<00:00, 60.37it/s]\n",
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 200/200 [00:03<00:00, 59.40it/s]\n",
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 200/200 [00:01<00:00, 112.83it/s]\n"
     ]
    }
   ],
   "source": [
    "def visualize_tracking(video_path, camera_id, trackings_global, output_path):\n",
    "    \"\"\"\n",
    "    Visualize tracking by drawing bounding boxes and track IDs on video frames for a specific camera.\n",
    "\n",
    "    Parameters:\n",
    "    - video_path: Path to the video file for the camera.\n",
    "    - camera_id: ID of the camera to visualize detections for.\n",
    "    - global_tracks: List of global tracks, each containing detections from multiple cameras.\n",
    "    - output_path: Path to save the output video with tracking visualization.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'MP4V'), frame_rate, (frame_width, frame_height))\n",
    "\n",
    "    for frame_number in tqdm(range(0, 200)):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Iterate through all tracks and their detections\n",
    "        for bbox in trackings_global[camera_id][frame_number]:\n",
    "            x1, y1, x2, y2, track_id = bbox\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 127, 0), 2)\n",
    "            cv2.putText(frame, str(track_id), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 127, 0), 2)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Load the video\n",
    "for c in range(0, len(camera_ids)):\n",
    "    video_path = video_paths[c]\n",
    "    camera_id = video_path.split('/')[-2]\n",
    "    output_path = f'tracking_output_{camera_id}.mp4' \n",
    "    visualize_tracking(video_path, camera_id, trackings_global, output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
