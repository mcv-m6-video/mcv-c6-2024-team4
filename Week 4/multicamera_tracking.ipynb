{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgudino/.virtualenv/hdtf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tkinter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msort\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sort\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     11\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/mnt/DATA/lgudino/C6/mcv-c6-2024-team4/Week 4/sort.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mmatplotlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTkAgg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpatches\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenv/hdtf/lib/python3.8/site-packages/matplotlib/__init__.py:1237\u001b[0m, in \u001b[0;36muse\u001b[0;34m(backend, force)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1233\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1234\u001b[0m         \u001b[38;5;66;03m# we need this import check here to re-raise if the\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m         \u001b[38;5;66;03m# user does not have the libraries to support their\u001b[39;00m\n\u001b[1;32m   1236\u001b[0m         \u001b[38;5;66;03m# chosen backend installed.\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m         \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswitch_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1238\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m   1239\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m force:\n",
      "File \u001b[0;32m~/.virtualenv/hdtf/lib/python3.8/site-packages/matplotlib/pyplot.py:271\u001b[0m, in \u001b[0;36mswitch_backend\u001b[0;34m(newbackend)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# have to escape the switch on access logic\u001b[39;00m\n\u001b[1;32m    269\u001b[0m old_backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(rcParams, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackend\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 271\u001b[0m backend_mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_module_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewbackend\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m required_framework \u001b[38;5;241m=\u001b[39m _get_required_interactive_framework(backend_mod)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenv/hdtf/lib/python3.8/site-packages/matplotlib/backends/backend_tkagg.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _backend_tk\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_agg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasAgg\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_backend_tk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _BackendTk, FigureCanvasTk\n",
      "File \u001b[0;32m~/.virtualenv/hdtf/lib/python3.8/site-packages/matplotlib/backends/_backend_tk.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtkinter\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtk\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtkinter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfiledialog\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtkinter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfont\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tkinter'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "from sort import Sort\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] contraseña para lgudino: \n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install python3-tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition remains the same\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, sequence):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights='ResNet50_Weights.DEFAULT')\n",
    "        self.model.fc = nn.Identity()\n",
    "        self.sequence = sequence\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def get_embedding(self, bbox, frame_num, camera_id):\n",
    "\n",
    "        name = 'vdo10.avi' if camera_id == 'c015' else 'vdo.avi'\n",
    "        path = f\"aic19-track1-mtmc-train/train/{self.sequence}/{camera_id}/{name}\"\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        frame = frame[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = torch.tensor((cv2.resize(frame, (224, 224)) / 255.0).transpose(2, 0, 1), dtype=torch.float32).unsqueeze(0)\n",
    "        return self.model(frame)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths = ['aic19-track1-mtmc-train/train/S01/c001/vdo.avi', \n",
    "               'aic19-track1-mtmc-train/train/S01/c002/vdo.avi', \n",
    "               'aic19-track1-mtmc-train/train/S01/c003/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S01/c004/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S01/c005/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S03/c010/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S03/c011/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S03/c012/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S03/c013/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S03/c014/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S03/c015/vdo10.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c016/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c017/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c018/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c019/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c020/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c021/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c022/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c023/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c024/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c025/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c026/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c027/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c028/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c029/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c030/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c031/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c032/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c033/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c034/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c035/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c036/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c037/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c038/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c039/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c040/vdo.avi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upscale_fps(input_path, output_path, input_fps=8, target_fps=10):\n",
    "    \"\"\"\n",
    "    This function upscales the FPS of a video by interpolating frames using OpenCV and NumPy.\n",
    "    \n",
    "    Parameters:\n",
    "    input_path (str): The path to the input video file.\n",
    "    output_path (str): The path where the output video will be saved.\n",
    "    input_fps (int): The original FPS of the video.\n",
    "    target_fps (int): The target FPS to upscale the video to.\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    \n",
    "    # Capture video from input path\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        return \"Failed to open video file.\"\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, target_fps, (width, height))\n",
    "    \n",
    "    prev_frame = None\n",
    "    success, frame = cap.read()\n",
    "    while success:\n",
    "        out.write(frame)  # Write the original frame\n",
    "        \n",
    "        # If there's a previous frame, generate interpolated frames\n",
    "        if prev_frame is not None:\n",
    "            # Generate interpolated frames\n",
    "            for _ in range((target_fps // input_fps) - 1):\n",
    "                interp_frame = cv2.addWeighted(prev_frame, 0.5, frame, 0.5, 0)\n",
    "                out.write(interp_frame)\n",
    "                \n",
    "        prev_frame = frame\n",
    "        success, frame = cap.read()\n",
    "        \n",
    "    # Release everything if job is finished\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    return \"Video FPS upscaled successfully.\"\n",
    "\n",
    "# upscale_fps('aic19-track1-mtmc-train/train/S03/c015/vdo.avi', 'aic19-track1-mtmc-train/train/S03/c015/vdo10.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_detections(video_path, yolo):\n",
    "    \"\"\"\n",
    "    Given a video, create detections for each frame in the video and store bounding boxes in a text file\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Create a text file to store the bounding boxes\n",
    "    f = open(f\"detections/{video_path.split('/')[-2].split('.')[0]}.txt\", \"w\")\n",
    "\n",
    "    frame_num = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Get the detections for the frame\n",
    "        detections = yolo(frame, verbose=False)\n",
    "\n",
    "        # Write the bounding boxes to the text file\n",
    "        for box in detections[0].boxes:\n",
    "            if box.cls == 2:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].int().tolist()\n",
    "                f.write(f\"{frame_num},{x1},{y1},{x2},{y2}\\n\")\n",
    "\n",
    "        frame_num += 1\n",
    "\n",
    "    f.close()\n",
    "    cap.release()\n",
    "\n",
    "# for video in tqdm(video_paths):\n",
    "#     create_detections(video, yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_detections_to_sort_format(detections):\n",
    "    \"\"\"\n",
    "    Convert detections to the format expected by the SORT algorithm. Stacks all the detections of the same frame into a single array.\n",
    "    \"\"\"\n",
    "    max_frame = detections[-1][0]\n",
    "    all_detections = []\n",
    "    for frame_num in range(max_frame + 1):\n",
    "        frame_detections = [d[1:] + [1] for d in detections if d[0] == frame_num]\n",
    "        if len(frame_detections) == 0:\n",
    "            # print(f\"No detections found for frame {frame_num}\")\n",
    "            frame_detections = np.empty((0, 5))\n",
    "        all_detections.append(frame_detections)\n",
    "    return all_detections\n",
    "\n",
    "def compute_similarity(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean distance between two embeddings.\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(embedding1 - embedding2)\n",
    "\n",
    "# def update_global_tracks(tracked_detections, camera_id, global_tracks, siamese_nn, frame_number, similarity_threshold=9):\n",
    "#     \"\"\"\n",
    "#     Update global tracks with new detections from a specific camera.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - tracked_detections: Detections from the current camera for a specific frame.\n",
    "#     - camera_id: Identifier for the current camera.\n",
    "#     - global_tracks: List of global tracks across all cameras.\n",
    "#     - siamese_nn: Siamese neural network model for embedding generation.\n",
    "#     - frame_number: The current frame number being processed.\n",
    "#     - similarity_threshold: Threshold for considering two detections as the same object.\n",
    "#     \"\"\"\n",
    "#     for detection in tracked_detections:\n",
    "#         # Generate embedding for the current detection using siamese_nn\n",
    "#         with torch.no_grad():\n",
    "#             detection_embedding = siamese_nn.get_embedding(detection, frame_number, camera_id)\n",
    "\n",
    "        \n",
    "#         best_match = None\n",
    "#         min_distance = float('inf')\n",
    "        \n",
    "#         # Attempt to find the best match for the detection in existing global tracks\n",
    "#         for track in global_tracks:\n",
    "#             if track['camera_id'] != camera_id:  # Only compare with tracks from other cameras\n",
    "#                 for track_embedding in track['embeddings']:\n",
    "#                     distance = compute_similarity(detection_embedding, track_embedding)\n",
    "#                     if distance < min_distance:\n",
    "#                         min_distance = distance\n",
    "#                         best_match = track\n",
    "        \n",
    "#         # If a match is found within the similarity threshold, update the track\n",
    "#         if min_distance <= similarity_threshold:\n",
    "#             best_match['detections'].append(detection)\n",
    "#             best_match['embeddings'].append(detection_embedding)\n",
    "#             best_match['last_seen'] = frame_number\n",
    "\n",
    "#         else:\n",
    "#             # No match found, create a new global track\n",
    "#             new_track = {\n",
    "#                 'track_id': len(global_tracks) + 1,\n",
    "#                 'detections': [detection],\n",
    "#                 'embeddings': [detection_embedding],\n",
    "#                 'camera_id': camera_id,\n",
    "#                 'last_seen': frame_number\n",
    "#             }\n",
    "#             global_tracks.append(new_track)\n",
    "\n",
    "#     return global_tracks\n",
    "\n",
    "def update_global_tracks(tracked_detections, camera_id, global_tracks, siamese_nn, frame_number, similarity_threshold=9):\n",
    "    \"\"\"\n",
    "    Update global tracks with new detections from a specific camera.\n",
    "    \n",
    "    Parameters:\n",
    "    - tracked_detections: Detections from the current camera for a specific frame.\n",
    "    - camera_id: Identifier for the current camera.\n",
    "    - global_tracks: List of global tracks across all cameras.\n",
    "    - siamese_nn: Siamese neural network model for embedding generation.\n",
    "    - frame_number: The current frame number being processed.\n",
    "    - similarity_threshold: Threshold for considering two detections as the same object.\n",
    "    \"\"\"\n",
    "    for detection in tracked_detections:\n",
    "        # Generate embedding for the current detection using siamese_nn\n",
    "        with torch.no_grad():\n",
    "            detection_embedding = siamese_nn.get_embedding(detection, frame_number, camera_id)\n",
    "\n",
    "        best_match = None\n",
    "        min_distance = float('inf')\n",
    "        \n",
    "        # Attempt to find the best match for the detection in existing global tracks\n",
    "        for track in global_tracks:\n",
    "            # No longer restrict to tracks from other cameras only\n",
    "            for track_embedding in track['embeddings']:\n",
    "                distance = compute_similarity(detection_embedding, track_embedding)\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    best_match = track\n",
    "        \n",
    "        # If a match is found within the similarity threshold, update the track\n",
    "        if min_distance <= similarity_threshold:\n",
    "            # Add detection to the best matching track without duplicating camera detections\n",
    "            if not any(d['camera_id'] == camera_id and d['frame_number'] == frame_number for d in best_match['detections']):\n",
    "                best_match['detections'].append({'detection': detection, 'camera_id': camera_id, 'frame_number': frame_number})\n",
    "                best_match['embeddings'].append(detection_embedding)\n",
    "                best_match['last_seen'] = frame_number\n",
    "        else:\n",
    "            # No match found, create a new global track\n",
    "            new_track = {\n",
    "                'track_id': len(global_tracks) + 1,\n",
    "                'detections': [{'detection': detection, 'camera_id': camera_id, 'frame_number': frame_number}],\n",
    "                'embeddings': [detection_embedding],\n",
    "                'camera_id': camera_id,  # This attribute is now less about exclusivity and more about the track's origin\n",
    "                'last_seen': frame_number\n",
    "            }\n",
    "            global_tracks.append(new_track)\n",
    "\n",
    "    return global_tracks\n",
    "\n",
    "def track_across_cameras(detections, camera_trackers, siamese_net):\n",
    "    global_tracks = []  # Initialize global tracks\n",
    "    \n",
    "    counter = 0\n",
    "    for camera_id, dets in detections.items():\n",
    "        dets_sort = convert_detections_to_sort_format(dets)\n",
    "        for frame_number, d in tqdm(enumerate(dets_sort), total=len(dets_sort)):\n",
    "            tracked_detections = camera_trackers[camera_id].update(np.array(d)).astype(int)\n",
    "            # keep bbox inside frame\n",
    "            tracked_detections = [[max(0, x) for x in det] for det in tracked_detections]\n",
    "            global_tracks = update_global_tracks(tracked_detections, camera_id, global_tracks, siamese_net, frame_number)\n",
    "\n",
    "            if (frame_number+1) % 150 == 0:\n",
    "                counter += 1\n",
    "                break\n",
    "        \n",
    "        if counter == 2:\n",
    "            break\n",
    "    # Step 3 (Optional): Handle cross-camera tracking logic\n",
    "    # This could involve additional steps to refine track matching across cameras,\n",
    "    # especially if tracks move from the view of one camera to another.\n",
    "    return global_tracks\n",
    "\n",
    "sequence = 'S01'\n",
    "if sequence == 'S01':\n",
    "    camera_ids = ['c001', 'c002', 'c003', 'c004', 'c005']\n",
    "elif sequence == 'S03':\n",
    "    camera_ids = ['c010', 'c011', 'c012', 'c013', 'c014', 'c015']\n",
    "elif sequence == 'S04':\n",
    "    camera_ids = ['c016', 'c017', 'c018', 'c019', 'c020', 'c021', 'c022', 'c023', 'c024', 'c025', 'c026', 'c027', 'c028', 'c029', 'c030', 'c031', 'c032', 'c033', 'c034', 'c035', 'c036', 'c037', 'c038', 'c039', 'c040']\n",
    "\n",
    "siamese_net = Net(sequence).eval()\n",
    "yolo = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Load detections from text files\n",
    "detections = {}\n",
    "for camera_id in camera_ids:\n",
    "    with open(f'detections/{camera_id}.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        detections[camera_id] = [[int(x) for x in line.strip().split(',')] for line in lines]\n",
    "\n",
    "# Initialization\n",
    "camera_trackers = {camera_id: Sort(iou_threshold=0.4) for camera_id in camera_ids}  # SORT tracker for each camera\n",
    "global_tracks = track_across_cameras(detections, camera_trackers, siamese_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:05<00:00, 27.48it/s]\n",
      "100%|██████████| 150/150 [00:10<00:00, 14.79it/s]\n"
     ]
    }
   ],
   "source": [
    "def visualize_tracking(video_path, camera_id, global_tracks, output_path):\n",
    "    \"\"\"\n",
    "    Visualize tracking by drawing bounding boxes and track IDs on video frames for a specific camera.\n",
    "\n",
    "    Parameters:\n",
    "    - video_path: Path to the video file for the camera.\n",
    "    - camera_id: ID of the camera to visualize detections for.\n",
    "    - global_tracks: List of global tracks, each containing detections from multiple cameras.\n",
    "    - output_path: Path to save the output video with tracking visualization.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'MP4V'), frame_rate, (frame_width, frame_height))\n",
    "\n",
    "    frame_number = 0\n",
    "    for i in tqdm(range(0, 150)):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Iterate through all tracks and their detections\n",
    "        for track in global_tracks:\n",
    "            for detection in track['detections']:\n",
    "                # Check if the detection is for the current frame and camera\n",
    "                if detection['camera_id'] == camera_id and detection['frame_number'] == frame_number:\n",
    "                    bbox = detection['detection']\n",
    "                    x, y, x2, y2 = bbox[:4]\n",
    "                    cv2.rectangle(frame, (x, y), (x2, y2), (0, 127, 0), 2)\n",
    "                    cv2.putText(frame, f\"ID: {track['track_id']}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 127, 0), 2)\n",
    "\n",
    "        out.write(frame)\n",
    "        frame_number += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Load the video\n",
    "for c in range(0,2):\n",
    "    video_path = video_paths[c]\n",
    "    camera_id = video_path.split('/')[-2]\n",
    "    output_path = f'tracking_output_{camera_id}.mp4' \n",
    "    visualize_tracking(video_path, camera_id, global_tracks, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_tracks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
