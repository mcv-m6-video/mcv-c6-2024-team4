{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "from sort import Sort\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition remains the same\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, sequence):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights='ResNet50_Weights.DEFAULT')\n",
    "        self.model.fc = nn.Identity()\n",
    "        self.sequence = sequence\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def get_embedding(self, bbox, frame_num, camera_id):\n",
    "\n",
    "        name = 'vdo10.avi' if camera_id == 'c015' else 'vdo.avi'\n",
    "        path = f\"aic19-track1-mtmc-train/train/{self.sequence}/{camera_id}/{name}\"\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        frame = frame[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = torch.tensor((cv2.resize(frame, (224, 224)) / 255.0).transpose(2, 0, 1), dtype=torch.float32).unsqueeze(0).cuda()\n",
    "        return self.model(frame)[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths = ['aic19-track1-mtmc-train/train/S01/c001/vdo.avi', \n",
    "               'aic19-track1-mtmc-train/train/S01/c002/vdo.avi', \n",
    "               'aic19-track1-mtmc-train/train/S01/c003/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S01/c004/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S01/c005/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S03/c010/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S03/c011/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S03/c012/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S03/c013/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S03/c014/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S03/c015/vdo10.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c016/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c017/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c018/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c019/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c020/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c021/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c022/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c023/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c024/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c025/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c026/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c027/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c028/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c029/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c030/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c031/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c032/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c033/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c034/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c035/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c036/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c037/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c038/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c039/vdo.avi',\n",
    "               'aic19-track1-mtmc-train/train/S04/c040/vdo.avi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upscale_fps(input_path, output_path, input_fps=8, target_fps=10):\n",
    "    \"\"\"\n",
    "    This function upscales the FPS of a video by interpolating frames using OpenCV and NumPy.\n",
    "    \n",
    "    Parameters:\n",
    "    input_path (str): The path to the input video file.\n",
    "    output_path (str): The path where the output video will be saved.\n",
    "    input_fps (int): The original FPS of the video.\n",
    "    target_fps (int): The target FPS to upscale the video to.\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    \n",
    "    # Capture video from input path\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        return \"Failed to open video file.\"\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, target_fps, (width, height))\n",
    "    \n",
    "    prev_frame = None\n",
    "    success, frame = cap.read()\n",
    "    while success:\n",
    "        out.write(frame)  # Write the original frame\n",
    "        \n",
    "        # If there's a previous frame, generate interpolated frames\n",
    "        if prev_frame is not None:\n",
    "            # Generate interpolated frames\n",
    "            for _ in range((target_fps // input_fps) - 1):\n",
    "                interp_frame = cv2.addWeighted(prev_frame, 0.5, frame, 0.5, 0)\n",
    "                out.write(interp_frame)\n",
    "                \n",
    "        prev_frame = frame\n",
    "        success, frame = cap.read()\n",
    "        \n",
    "    # Release everything if job is finished\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    return \"Video FPS upscaled successfully.\"\n",
    "\n",
    "# upscale_fps('aic19-track1-mtmc-train/train/S03/c015/vdo.avi', 'aic19-track1-mtmc-train/train/S03/c015/vdo10.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_detections(video_path, yolo):\n",
    "    \"\"\"\n",
    "    Given a video, create detections for each frame in the video and store bounding boxes in a text file\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Create a text file to store the bounding boxes\n",
    "    f = open(f\"detections/{video_path.split('/')[-2].split('.')[0]}.txt\", \"w\")\n",
    "\n",
    "    frame_num = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Get the detections for the frame\n",
    "        detections = yolo(frame, verbose=False)\n",
    "\n",
    "        # Write the bounding boxes to the text file\n",
    "        for box in detections[0].boxes:\n",
    "            if box.cls == 2:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].int().tolist()\n",
    "                f.write(f\"{frame_num},{x1},{y1},{x2},{y2}\\n\")\n",
    "\n",
    "        frame_num += 1\n",
    "\n",
    "    f.close()\n",
    "    cap.release()\n",
    "\n",
    "# for video in tqdm(video_paths):\n",
    "#     create_detections(video, yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_detections_to_sort_format(detections):\n",
    "    \"\"\"\n",
    "    Convert detections to the format expected by the SORT algorithm. Stacks all the detections of the same frame into a single array.\n",
    "    \"\"\"\n",
    "    max_frame = detections[-1][0]\n",
    "    all_detections = []\n",
    "    for frame_num in range(max_frame + 1):\n",
    "        frame_detections = [d[1:] + [1] for d in detections if d[0] == frame_num]\n",
    "        if len(frame_detections) == 0:\n",
    "            # print(f\"No detections found for frame {frame_num}\")\n",
    "            frame_detections = np.empty((0, 5))\n",
    "        all_detections.append(frame_detections)\n",
    "    return all_detections\n",
    "\n",
    "def compute_similarity(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean distance between two embeddings.\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(embedding1 - embedding2)\n",
    "\n",
    "sequence = 'S01'\n",
    "if sequence == 'S01':\n",
    "    camera_ids = ['c001', 'c002', 'c003', 'c004', 'c005']\n",
    "elif sequence == 'S03':\n",
    "    camera_ids = ['c010', 'c011', 'c012', 'c013', 'c014', 'c015']\n",
    "elif sequence == 'S04':\n",
    "    camera_ids = ['c016', 'c017', 'c018', 'c019', 'c020', 'c021', 'c022', 'c023', 'c024', 'c025', 'c026', 'c027', 'c028', 'c029', 'c030', 'c031', 'c032', 'c033', 'c034', 'c035', 'c036', 'c037', 'c038', 'c039', 'c040']\n",
    "\n",
    "siamese_net = Net(sequence).cuda().eval()\n",
    "siamese_net.load_state_dict(torch.load('siamese_model_6.pth', map_location=torch.device('cuda')))\n",
    "# yolo = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Load detections from text files\n",
    "detections = {}\n",
    "for camera_id in camera_ids:\n",
    "    with open(f'detections/{camera_id}.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        detections[camera_id] = [[int(x) for x in line.strip().split(',')] for line in lines]\n",
    "\n",
    "# Compute single trackings for each camera\n",
    "trackings = {c_id: [] for c_id in camera_ids}\n",
    "for camera_id, dets in tqdm(detections.items(), desc=\"Computing trackings\"):\n",
    "    dets_sort = convert_detections_to_sort_format(dets)\n",
    "    mot_tracker = Sort()\n",
    "    for frame_num, frame_dets in enumerate(dets_sort):\n",
    "        d = mot_tracker.update(np.array(frame_dets)).astype(int)\n",
    "        d = [[max(0, x) for x in det] for det in d]\n",
    "        trackings[camera_id].append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compute all embeddings for each detection in every camera and frame\n",
    "precomputed_embeddings = {}\n",
    "for cam_id in camera_ids:\n",
    "    precomputed_embeddings[cam_id] = {}\n",
    "    for frame, detections in tqdm(enumerate(trackings[cam_id]), desc=f\"Precomputing embeddings for {cam_id}\", total=len(trackings[cam_id])):\n",
    "        with torch.no_grad():\n",
    "            precomputed_embeddings[cam_id][frame] = [\n",
    "                siamese_net.get_embedding(det[:4], frame, cam_id) for det in detections\n",
    "            ]\n",
    "\n",
    "with open('precomputed_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(precomputed_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('precomputed_embeddings.pkl', 'rb') as f:\n",
    "    precomputed_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize matches dictionary\n",
    "matches = {}\n",
    "\n",
    "# Only iterate through the first two camera IDs for pairwise comparison\n",
    "for i, cam1 in enumerate(camera_ids[:2]):\n",
    "    for cam2 in camera_ids[i+1:2]:\n",
    "        if cam1 == cam2:\n",
    "            continue  # Skip if cameras are the same, though this is already ensured by the loop logic\n",
    "\n",
    "        matches[(cam1, cam2)] = []\n",
    "\n",
    "        # Iterate through frames and detections for the first camera\n",
    "        for frame1, detections1 in tqdm(enumerate(trackings[cam1]), desc=f\"Matching {cam1} and {cam2}\", total=len(trackings[cam1])):\n",
    "            for frame2, detections2 in enumerate(trackings[cam2]):\n",
    "\n",
    "                # Utilize the break condition more efficiently\n",
    "                if abs(frame1 - frame2) > 150:\n",
    "                    continue\n",
    "\n",
    "                # Compute matches using precomputed embeddings\n",
    "                for idx1, det1 in enumerate(detections1):\n",
    "                    for idx2, det2 in enumerate(detections2):\n",
    "                        embedding1 = precomputed_embeddings[cam1][frame1][idx1]\n",
    "                        embedding2 = precomputed_embeddings[cam2][frame2][idx2]\n",
    "                        similarity = compute_similarity(embedding1, embedding2)\n",
    "\n",
    "                        if similarity < 3.3:\n",
    "                            matches[(cam1, cam2)].append((frame1, frame2, det1, det2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: Initialize Global Tracking\n",
    "global_tracking = defaultdict(lambda: defaultdict(list))\n",
    "global_id_counter = 0\n",
    "\n",
    "# Step 2: Assign Global Identifiers to Matched Detections Across Videos\n",
    "for (cam1, cam2), matched_detections in matches.items():\n",
    "    for frame1, frame2, det1, det2 in tqdm(matched_detections, desc=f\"Assigning global IDs for {cam1} and {cam2}\", total=len(matched_detections)):\n",
    "        # Check if either detection already has a global ID; if not, assign a new one\n",
    "        existing_global_id = None\n",
    "        for gid, tracks in global_tracking.items():\n",
    "            # Corrected search within the lists of tuples for each camera\n",
    "            if any((frame1, det1) == tracking for tracking in tracks[cam1]) or any((frame2, det2) == tracking for tracking in tracks[cam2]):\n",
    "                existing_global_id = gid\n",
    "                break\n",
    "        \n",
    "        if existing_global_id is None:\n",
    "            existing_global_id = global_id_counter\n",
    "            global_id_counter += 1\n",
    "        \n",
    "        # Add the detections to the global tracking under the found or new global ID\n",
    "        global_tracking[existing_global_id][cam1].append((frame1, det1))\n",
    "        global_tracking[existing_global_id][cam2].append((frame2, det2))\n",
    "\n",
    "# Step 3: Merge Within-Video Trackings\n",
    "for cam, tracks in trackings.items():\n",
    "    for frame, detections in tqdm(enumerate(tracks), desc=f\"Merging within-video trackings for {cam}\", total=len(tracks)):\n",
    "        for det in detections:\n",
    "            # Check if this detection is already in global tracking\n",
    "            already_tracked = False\n",
    "            for gid, t in global_tracking.items():\n",
    "                if any((frame, det) == ti for ti in t[cam]):\n",
    "                    already_tracked = True\n",
    "                    break\n",
    "            # If not already tracked, assign a new global ID\n",
    "            if not already_tracked:\n",
    "                global_tracking[global_id_counter][cam].append((frame, det))\n",
    "                global_id_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_ids = ['c001', 'c002', 'c003', 'c004', 'c005']\n",
    "# Load detections from text files\n",
    "detections = {}\n",
    "for camera_id in camera_ids:\n",
    "    with open(f'detections/{camera_id}.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        detections[camera_id] = [[int(x) for x in line.strip().split(',')] for line in lines]\n",
    "trackers = initialize_trackers(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackers['c005'].trackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tracking(video_path, camera_id, global_tracks, output_path):\n",
    "    \"\"\"\n",
    "    Visualize tracking by drawing bounding boxes and track IDs on video frames for a specific camera.\n",
    "\n",
    "    Parameters:\n",
    "    - video_path: Path to the video file for the camera.\n",
    "    - camera_id: ID of the camera to visualize detections for.\n",
    "    - global_tracks: List of global tracks, each containing detections from multiple cameras.\n",
    "    - output_path: Path to save the output video with tracking visualization.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'MP4V'), frame_rate, (frame_width, frame_height))\n",
    "\n",
    "    frame_number = 0\n",
    "    for i in tqdm(range(0, 150)):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Iterate through all tracks and their detections\n",
    "        for track in global_tracks:\n",
    "            for detection in track['detections']:\n",
    "                # Check if the detection is for the current frame and camera\n",
    "                if detection['camera_id'] == camera_id and detection['frame_number'] == frame_number:\n",
    "                    bbox = detection['bbox']\n",
    "                    x, y, x2, y2 = bbox\n",
    "                    cv2.rectangle(frame, (x, y), (x2, y2), (0, 127, 0), 2)\n",
    "                    cv2.putText(frame, f\"ID: {track['track_id']}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 127, 0), 2)\n",
    "\n",
    "        out.write(frame)\n",
    "        frame_number += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Load the video\n",
    "for c in range(0,2):\n",
    "    video_path = video_paths[c]\n",
    "    camera_id = video_path.split('/')[-2]\n",
    "    output_path = f'tracking_output_{camera_id}.mp4' \n",
    "    visualize_tracking(video_path, camera_id, global_tracks, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
