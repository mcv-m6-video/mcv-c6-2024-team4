{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import extract_rectangles_from_xml\n",
    "from evaluation import mAP\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = extract_rectangles_from_xml('data/ai_challenge_s03_c010-full_annotation.xml')\n",
    "parked_cars = annotation[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame count: 2141 FPS: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 535/535 [00:30<00:00, 17.46it/s]\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture('data/AICity_data/train/S03/c010/vdo.avi')\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print('Frame count:', frame_count, 'FPS:', fps)\n",
    "\n",
    "split_frame = frame_count // 4\n",
    "\n",
    "# Choose color space\n",
    "color_space = \"lab\"\n",
    "if color_space == \"rgb\":\n",
    "    num_gaussians = 3\n",
    "    color_transform = cv2.COLOR_BGR2RGB\n",
    "elif color_space == \"lab\":\n",
    "    num_gaussians = 2\n",
    "    color_transform = cv2.COLOR_BGR2LAB\n",
    "if color_space == \"grayscale\":\n",
    "    num_gaussians = 1\n",
    "    color_transform = cv2.COLOR_BGR2GRAY\n",
    "\n",
    "\n",
    "# Initialize cumulative sum and sum of squares\n",
    "cum_sum = np.zeros((height, width, num_gaussians), dtype=np.float64)\n",
    "cum_sum_sq = np.zeros((height, width, num_gaussians), dtype=np.float64)\n",
    "\n",
    "for i in tqdm.tqdm(range(split_frame)):  # Process 25% of the frames\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    color_frame = cv2.cvtColor(frame, color_transform).astype(np.float64)\n",
    "    if color_space == \"lab\":\n",
    "        color_frame = color_frame[:, :, 1:]\n",
    "    elif color_space == \"grayscale\":\n",
    "        color_frame = color_frame[:, :, None]\n",
    "    cum_sum += color_frame\n",
    "    cum_sum_sq += color_frame ** 2\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Calculate mean and variance\n",
    "mean = cum_sum / split_frame\n",
    "variance = (cum_sum_sq / split_frame) - (mean ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shadow_detection(image):\n",
    "    # Future work, implement the shadow detection with brightness and \n",
    "    # color detection\n",
    "    return 0, 0\n",
    "\n",
    "def remove_shadow_gabor(image):\n",
    "    # Gabor filter parameters\n",
    "    num = 8  # Number of different orientations\n",
    "    vects = 8  # Number of different wavelengths (vector sizes)\n",
    "\n",
    "    gabor_features = np.zeros((image.shape[0], image.shape[1], num * vects), dtype=np.double)\n",
    "\n",
    "    for i in range(num):\n",
    "        theta = i / num * np.pi\n",
    "        for j in range(vects):\n",
    "            lamda = int(image.shape[0] / (2 ** j))\n",
    "            g_kernel = cv2.getGaborKernel((lamda, lamda), sigma=4.0, theta=theta, lambd=lamda, gamma=0.5)\n",
    "            filtered_img = cv2.filter2D(image, cv2.CV_8UC3, g_kernel)\n",
    "            gabor_features[:, :, i * vects + j] = filtered_img\n",
    "\n",
    "    gabor_features_binary = (gabor_features.mean(axis=2) > 2.5*gabor_features.mean(axis=2).mean()).astype(np.uint8)\n",
    "\n",
    "    \n",
    "    # Find the columns that have non-zero values\n",
    "    non_zero_columns = np.where(gabor_features_binary.sum(axis=0) > 0)[0]\n",
    "\n",
    "    if len(non_zero_columns) == 0:\n",
    "        return 0, image.shape[1]\n",
    "    # The minimum and maximum x values with non-zero values (bbox horizontal)\n",
    "    min_x = non_zero_columns.min()\n",
    "    max_x = non_zero_columns.max()\n",
    "\n",
    "    return min_x, max_x\n",
    "\n",
    "# Function to calculate if rectangles a and b are close\n",
    "def are_close(a, b, proximity_threshold):\n",
    "    left_a, top_a, right_a, bottom_a = a\n",
    "    left_b, top_b, right_b, bottom_b = b\n",
    "\n",
    "    # Check if rectangles are close based on the threshold\n",
    "    horizontal_close = (left_b <= right_a + proximity_threshold and right_b >= left_a - proximity_threshold)\n",
    "    vertical_close = (top_b <= bottom_a + proximity_threshold and bottom_b >= top_a - proximity_threshold)\n",
    "\n",
    "    return horizontal_close and vertical_close\n",
    "\n",
    "# Function to merge two rectangles\n",
    "def merge_rects(a, b):\n",
    "    left_a, top_a, right_a, bottom_a = a\n",
    "    left_b, top_b, right_b, bottom_b = b\n",
    "    return (min(left_a, left_b), min(top_a, top_b), max(right_a, right_b), max(bottom_a, bottom_b))\n",
    "\n",
    "def merge_close_rectangles(rectangles, proximity_threshold):\n",
    "    # Convert rectangles to a format that includes the bottom-right corner for easier comparison\n",
    "    rects_with_br = [(x, y, x+w, y+h) for x, y, w, h in rectangles]\n",
    "\n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged = False\n",
    "        new_rects = []\n",
    "        while rects_with_br:\n",
    "            current = rects_with_br.pop(0)\n",
    "            for i, other in enumerate(rects_with_br):\n",
    "                if are_close(current, other, proximity_threshold):\n",
    "                    new_rect = merge_rects(current, other)\n",
    "                    rects_with_br[i] = new_rect  # Replace the \"other\" rect with the merged one\n",
    "                    current = new_rect  # Update current to be the merged rect\n",
    "                    merged = True\n",
    "                    break\n",
    "            else:\n",
    "                new_rects.append(current)  # Add current rect if it wasn't merged\n",
    "        rects_with_br = new_rects  # Update list with merged rects\n",
    "\n",
    "    # Convert back to original format\n",
    "    merged_rectangles = [(left, top, right-left, bottom-top) for left, top, right, bottom in rects_with_br]\n",
    "    return merged_rectangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_bbox = [\n",
    "    [list(np.array(r).astype(int)) for r in rect if r not in parked_cars]\n",
    "    for rect in list(annotation.values())[split_frame:]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 3\n",
      "Rho: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frames 0 to 100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 1/16 [01:00<15:13, 60.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frames 100 to 200...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 2/16 [01:58<13:42, 58.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frames 200 to 300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 3/16 [02:56<12:40, 58.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frames 300 to 400...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 4/16 [03:52<11:33, 57.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frames 400 to 500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 5/16 [04:51<10:38, 58.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frames 500 to 600...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 6/16 [05:57<10:06, 60.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frames 600 to 700...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 7/16 [07:07<09:33, 63.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frames 700 to 800...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 8/16 [08:14<08:38, 64.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frames 800 to 900...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 9/16 [09:20<07:36, 65.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frames 900 to 1000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 10/16 [10:24<06:28, 64.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frames 1000 to 1100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 11/16 [11:29<05:24, 64.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frames 1100 to 1200...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 12/16 [12:39<04:25, 66.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frames 1200 to 1300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 13/16 [13:49<03:22, 67.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frames 1300 to 1400...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 14/16 [14:57<02:15, 67.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frames 1400 to 1500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 15/16 [16:04<01:07, 67.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frames 1500 to 1600...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [17:12<00:00, 64.50s/it]\n"
     ]
    }
   ],
   "source": [
    "def process_frame_adaptative(frame, mean, variance, alpha, rho, color_space=\"lab\"):\n",
    "    \"\"\"Process a single frame to extract foreground bounding boxes.\"\"\"\n",
    "    assert color_space in [\"lab\", \"rgb\", \"grayscale\"], 'Choose colorspace in [\"lab\", \"rgb\", \"grayscale\"]'\n",
    "    if color_space == \"lab\":\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)[:, :, 1:]\n",
    "        alphas, rhos = np.full(2, alpha), np.full(2, rho)\n",
    "    elif color_space == \"rgb\":\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        alphas, rhos = np.full(3, alpha), np.full(3, rho)\n",
    "    elif color_space == \"grayscale\":\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)[:,:,None]\n",
    "        alphas, rhos = np.full(1, alpha), np.full(1, rho)\n",
    "\n",
    "    abs_diff = np.abs(frame - mean)\n",
    "\n",
    "    intensity = abs_diff.sum(axis=(0,1))\n",
    "    for i, intensity in enumerate(intensity):\n",
    "        if intensity <= 25000000:\n",
    "            alphas[i] *= 0.5\n",
    "        elif intensity >= 55000000:\n",
    "            alphas[i] *= 1.5\n",
    "\n",
    "    foreground_mask = abs_diff >= alphas * (np.sqrt(variance) + 2)\n",
    "    foreground_binary = np.where(foreground_mask, 255, 0).astype(np.uint8)\n",
    "\n",
    "    background_mask = ~foreground_mask  # Inverting the foreground mask to get the background\n",
    "    for c in range(frame.shape[2]):\n",
    "        mean[background_mask[:,:,c], c] = rhos[c] * frame[background_mask[:,:,c], c] + (1 - rhos[c]) * mean[background_mask[:,:,c], c]\n",
    "        variance[background_mask[:,:,c], c] = rhos[c] * ((frame[background_mask[:,:,c], c] - mean[background_mask[:,:,c], c]) ** 2) + (1 - rhos[c]) * variance[background_mask[:,:,c], c]\n",
    "    \n",
    "    foreground_clean = np.zeros(frame.shape)\n",
    "    for c in range(frame.shape[2]):\n",
    "        foreground_clean[:, :, c] = cv2.morphologyEx(foreground_binary[:, :, c], cv2.MORPH_CLOSE, np.ones((5, 5), np.uint8))\n",
    "        foreground_clean[:, :, c] = cv2.morphologyEx(foreground_clean[:, :, c], cv2.MORPH_OPEN, np.ones((7,7), np.uint8))\n",
    "\n",
    "    if color_space == \"lab\":\n",
    "        foreground_clean = cv2.bitwise_and(foreground_clean[:,:,0], foreground_clean[:,:,1]).astype(np.uint8)\n",
    "    elif color_space == \"rgb\":\n",
    "        foreground_clean_aux = cv2.bitwise_and(foreground_clean[:,:,0], foreground_clean[:,:,1])\n",
    "        foreground_clean = cv2.bitwise_and(foreground_clean_aux, foreground_clean[:,:,2]).astype(np.uint8)\n",
    "    elif color_space == \"grayscale\":\n",
    "        foreground_clean = foreground_clean[:,:,0].astype(np.uint8)\n",
    "\n",
    "    contours, _ = cv2.findContours(foreground_clean, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    rectangles_merged = merge_close_rectangles([cv2.boundingRect(contour) for contour in contours], 20)\n",
    "    \n",
    "    rectangles_output = []\n",
    "    for i, (x, y, w, h) in enumerate(rectangles_merged):\n",
    "        if w < 80 or h < 80:\n",
    "            continue\n",
    "        \n",
    "        if color_space == \"grayscale\":\n",
    "            new_xmin = remove_shadow_gabor(frame[y:y+h, x:x+w])[0]\n",
    "            rectangles_output.append([x + new_xmin, y, x + w, y + h])\n",
    "            foreground_clean[y:y+h, x:x+new_xmin] = 0\n",
    "\n",
    "        elif color_space == \"lab\":\n",
    "            rectangles_output.append([x, y, x + w, y + h])\n",
    "\n",
    "        elif color_space == \"rgb\":\n",
    "            new_xmin = shadow_detection(frame[y:y+h, x:x+w])[0]\n",
    "            rectangles_output.append([x + new_xmin, y, x + w, y + h])\n",
    "            foreground_clean[y:y+h, x:x+new_xmin] = 0\n",
    "\n",
    "    return rectangles_output, foreground_binary, foreground_clean, mean, variance\n",
    "\n",
    "def process_frame(frame, mean, variance, alpha):\n",
    "    \"\"\"Process a single frame to extract foreground bounding boxes.\"\"\"\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    abs_diff = np.abs(gray_frame - mean)\n",
    "    \n",
    "    intensity = abs_diff.sum()\n",
    "    if intensity <= 25000000:\n",
    "        alpha *= 0.5\n",
    "    elif intensity >= 55000000:\n",
    "        alpha *= 1.5\n",
    "\n",
    "    foreground_mask = abs_diff >= alpha * (np.sqrt(variance) + 2)\n",
    "    foreground_binary = np.where(foreground_mask, 255, 0).astype(np.uint8)\n",
    "    \n",
    "    foreground_clean = cv2.morphologyEx(foreground_binary, cv2.MORPH_CLOSE, np.ones((5, 5), np.uint8))\n",
    "    foreground_clean = cv2.morphologyEx(foreground_clean, cv2.MORPH_OPEN, np.ones((7,7), np.uint8))\n",
    "\n",
    "    contours, _ = cv2.findContours(foreground_clean, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    rectangles_merged = merge_close_rectangles([cv2.boundingRect(contour) for contour in contours], 20)\n",
    "    \n",
    "    rectangles_output = []\n",
    "    for i, (x, y, w, h) in enumerate(rectangles_merged):\n",
    "\n",
    "        if w < 80 or h < 80:\n",
    "            continue\n",
    "\n",
    "        new_xmin = remove_shadow_gabor(gray_frame[y:y+h, x:x+w])[0]\n",
    "        rectangles_output.append([x + new_xmin, y, x + w, y + h])\n",
    "\n",
    "        foreground_clean[y:y+h, x:x+new_xmin] = 0\n",
    "\n",
    "    return rectangles_output, foreground_binary, foreground_clean\n",
    "\n",
    "def process_video(video_path, split_frame, frame_count, mean, variance, gt_bbox, alpha, rho=None, adaptative=False, color_space=\"lab\"):\n",
    "    \"\"\"Process the video to overlay predicted and ground truth bounding boxes.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    mAPs = []\n",
    "    \n",
    "    for n in tqdm.tqdm(range(split_frame, (frame_count//100)*100, 100)):\n",
    "        print(f'Processing frames {n-split_frame} to {min(n+100, frame_count)-split_frame}...')\n",
    "        out = cv2.VideoWriter(f'media/{color_space.upper()}_output_{n-split_frame}.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 10, (frame_width, frame_height))\n",
    "        out3 = cv2.VideoWriter(f'media/{color_space.upper()}_output_{n-split_frame}_clean.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 10, (frame_width, frame_height), isColor=False)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, n)\n",
    "        pred_bbox = []\n",
    "\n",
    "        for _ in range(n, min(n+100, frame_count)):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if adaptative:\n",
    "                bbox, binary, clean, mean, variance = process_frame_adaptative(frame, mean, variance, alpha, rho, color_space)\n",
    "            else:\n",
    "                bbox, binary, clean = process_frame(frame, mean, variance, alpha)\n",
    "            \n",
    "            pred_bbox.append(bbox)\n",
    "            out3.write(clean)\n",
    "\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, n)\n",
    "        for i, _ in enumerate(pred_bbox):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            for rect in pred_bbox[i]:\n",
    "                cv2.rectangle(frame, (rect[0], rect[1]), (rect[2], rect[3]), (0, 0, 255), 2)\n",
    "            # Assuming gt_bbox is defined elsewhere and accessible here\n",
    "            for rect in gt_bbox[n + i - split_frame]:\n",
    "                cv2.rectangle(frame, (rect[0], rect[1]), (rect[2], rect[3]), (0, 255, 0), 2)\n",
    "\n",
    "            out.write(frame)\n",
    "        m = mAP(gt_bbox[(n - split_frame):(min(n+100, frame_count)- split_frame)], pred_bbox)\n",
    "        mAPs.append(m)\n",
    "        \n",
    "        out.release()\n",
    "        #out2.release()\n",
    "        out3.release()\n",
    "    cap.release()\n",
    "    return mAPs\n",
    "\n",
    "mean_mAPs = []\n",
    "for rho in [0.005]:#, 0.1, 0.2, 0.3, 0.4]:\n",
    "    for alpha in [3]:#, 3, 4, 5]:\n",
    "        print(f'Alpha: {alpha}')\n",
    "        print(f'Rho: {rho}')\n",
    "        mean_mAPs.append(np.mean(process_video('data/AICity_data/train/S03/c010/vdo.avi', split_frame, frame_count, mean, variance, gt_bbox, alpha, rho, adaptative=True, color_space=color_space)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2601229030526333]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_mAPs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
